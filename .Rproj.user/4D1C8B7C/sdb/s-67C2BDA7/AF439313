{
    "contents" : "# Author: Brian Waismeyer\n# Contact: bwaismeyer@gmail.com\n\n# Date created: 3/25/2015\n# Date updated: 7/6/2015\n\n###############################################################################\n## SCRIPT OVERVIEW\n\n# GOAL: This script provides the suite of custom functions that allow the \n#       Multinomial Outcome Simulator (MOS) to generate and manage the \n#       simulations, visualizations, and user interactions that make up the \n#       application.\n#\n#       The simulation process was heavily inspired by code authored by\n#       Chris Adolph (http://faculty.washington.edu/cadolph/) and this script\n#       makes use of original and modified versions of this code in several\n#       places.\n#\n#       Where to find the original example code:\n#       library(simcf)\n#       help(mlogitsimev)\n#\n#       The original code was not designed for the generalizations or end uses\n#       needed by the MOS. It has been corrected, extended, and made into\n#       a function set (performance testable).\n#\n#       Given the importance of the simulation process to the MOS, here is a\n#       quick outline of it:\n#       - take a given data set and model formula and fit a multinomial model\n#         to the data\n#       - sample a range of values for each model coefficient with respect to \n#         the uncertainty associated with each coeffecient\n#       - based on user inputs, generate a set of counterfactuals - a collection\n#         of example cases that span a user-defined range (e.g., if an x-axis\n#         is selected, we want a data frame where the x-axis variable varies\n#         and all other variables are fixed to a reasonable value)\n#       - pass these example cases through our sampled model coefficients, \n#         generated likelihood estimates for each outcome for each case\n#       - visualize the relationship between the likelihoods and the outcomes\n#         with respect to user inputs (this may or may not require that the\n#         likelihood estimates be summarized for each case)\n#\n# SCRIPT OUTLINE:\n# - Load Supporting Packages\n#\n# - Functions to Simulate Outcome Likelihoods\n#   - The family of functions that used to go from base data to a data frame\n#     of outcome likelihoods that can be visualized.\n#\n# - Functions to Visualize Outcome Likelihoods\n#   - The family of functions that re-formats (if needed) the likelihood\n#     data frames and generates the relevant visualizations (ribbon plot\n#     and dot cloud plot).\n#\n# - Assorted Helper Functions\n#   - A function that makes easy-to-access lists of variable names associated\n#     with fixed UI options (e.g., which variables are x-axis candidates).\n#   - A function to identify which columns in the expanded data frame represent\n#     interactions.\n#   - A trio of functions to generate additional definitions for user inputs,\n#     create the user inputs, and apply the values from the user inputs.\n#   - A function to define the ribbon plot summaries based on variable\n#     configuration features.\n#   - A function to simply request all the values for the input collections.\n#     This is primarily useful for testing if the inputs have been interacted\n#     with.\n\n###############################################################################\n## Load Supporting Packages\n#   - devtools and packrat are highly useful to support MOS instances - I\n#     suggest installing, learning, and using these applications\n# install.packages(c(\"devtools\", \"packrat\"))\n\n#   - To install the batch of packages below, except for simcf:\n# package_names <- c(\"shiny\", \"shinyBS\", \"shinythemes\", \"Cairo\", \"nnet\",\n#                    \"combinat\", \"MASS\", \"dplyr\", \"reshape2\", \"tidyr\",\n#                    \"ggplot2\", \"scales\", \"grid\")\n# install.packages(package_names)\n\n#   - To get simcf (a custom package on github), use the install_github \n#     function from the devtools package:\n# devtools::install_github(\"chrisadolph/tile-simcf\", subdir = \"simcf\")\n\nlibrary(shiny)         # insuring that Shiny is packrat friendly and loaded\nlibrary(shinyBS)       # expands base Shiny features (e.g., popovers)\nlibrary(shinythemes)   # allows using basic bootswatch without custom CSS file\nlibrary(Cairo)         # supports improved plot quality across devices\nlibrary(nnet)          # for fitting multinomial logit model\nlibrary(combinat)      # for building interaction term permutations\nlibrary(MASS)          # for multivariate sampling\nlibrary(simcf)         # for creating counterfactual sets\nlibrary(dplyr)         # serves various formatting needs\nlibrary(reshape2)      # serves various formatting needs\nlibrary(tidyr)         # serves various formatting needs\nlibrary(ggplot2)       # our data visualization workhorse\nlibrary(scales)        # for nice plot scales\nlibrary(grid)          # need for \"unit\" function passed to ggplot\n\n###############################################################################\n## Functions to Simulate Outcome Likelihoods\n#   - create an expanded version of the source dataset (using same model as\n#     will be used in the multinom fitting)\n#   - fit the multinom model\n#   - extract point estimates from the model object\n#   - solve the model object Hessian matrix for the covariance matrix\n#   - get coefficient estimates via simulation\n#   - generate data to feed to coefficient estimates\n#   - get the outcome predictions (feed the estimates!)\n\n# create an expanded version of the source dataset\n# simply use model.matrix(formula, dataset)\n\n# fit the multinom model\n# simply use multinom(formula, dataset, Hess = T)\n\n# function to extract non-reference point estimates from the model object\nget_point_estimates <- function(model_object) {\n    # determine the number of coefficents (intercepts, predictors, interactions) \n    # and outcomes\n    number_coefficients <- length(model_object$coefnames)\n    number_outcomes <- length(model_object$lab)\n    \n    # the multinom function returns a lot of 0s - first we find identify where \n    # the non-reference weights begin (one set of weights per outcome but we \n    # skip the initial reference set)\n    index_starts <- NULL\n    for(i in 1:(number_outcomes - 1)) {\n        # go the end of the current chunk... add two to skip the placeholder 0...\n        index_starts[i] <- i * (number_coefficients + 1) + 2\n    }\n    \n    # then we use the start indices and the the number of coefficients to\n    # define the index that will align with all the non-reference weights\n    wts_index <- NULL\n    for(i in index_starts) {\n        wts_index <- c(wts_index, i:(i + number_coefficients - 1))\n    }\n    \n    # finally we return the point estimates (weights) for the coefficients\n    return(model_object$wts[wts_index])\n}\n\n# function to get the covariance matrix from the Hessian in the model object\nget_covariance_matrix <- function(model_object) {\n    cov_matrix <- chol2inv(chol(model_object$Hess))\n    dimnames(cov_matrix) <- dimnames(model_object$Hess)\n    \n    return(cov_matrix)\n}\n\n# function to get coefficient estimates via simulation\nget_coefficient_estimates <- function(sample_size, \n                                      point_estimates, \n                                      covariance_matrix,\n                                      model_object) {\n    # draw parameters, using MASS::mvrnorm\n    sim_betas <- mvrnorm(sample_size, point_estimates, covariance_matrix)\n    \n    # data needs to be re-arranged into an array format\n    # first determine array dimensions...\n    # looks crazy, but we're essentially taking all the UNIQUE variables in\n    # the model formula then subtract 1 for the outcome variable and 1 for the\n    # reference variable\n    number_arrays  <- length(model_object$lab) - 1\n    number_columns <- length(point_estimates) / number_arrays\n    number_rows    <- sample_size\n    \n    # then re-arrange simulates to array format for MNL simulation\n    sim_beta_array <- array(NA, dim = c(number_rows, \n                                        number_columns, \n                                        number_arrays)\n    ) \n    \n    index_starts <- seq(from = 1, to = number_columns * number_arrays, \n                        by = number_columns)\n    for(i in 1:number_arrays) {\n        sim_beta_array[, , i] <- sim_betas[, index_starts[i]:(index_starts[i] \n                                                              + number_columns -\n                                                                  1)]\n    }\n    \n    # return the re-arranged coefficient estimates\n    return(sim_beta_array)\n}\n\n# generate counterfactual cases to feed the coefficient estimates\nget_cf_cases <- function(exp_data, \n                         base_data, \n                         model_object,\n                         x_axis_selected, \n                         x_range = NULL, \n                         x_range_density = 100,\n                         facet_selected = NULL,\n                         interaction_col_names = NA) {\n    \n    # check if an explicit range has been provided for the x-axis variable\n    if(is.null(x_range)) {\n        # if not provided, calculate the range from the dataset\n        # floor and ceiling used to insure some space around the observed data\n        x_range[1] <- min(exp_data[x_axis_selected])\n        x_range[2] <- max(exp_data[x_axis_selected])\n    }\n    \n    # initialize the minimum set of counterfactuals (the x-axis variable cuts)\n    counterfactuals <- seq(x_range[1], x_range[2], length.out = x_range_density)\n    # check if a facet variable has been set\n    if(!is.null(facet_selected)) {\n        # if a facet variable has been set, we expand the counterfactuals\n        # to include all x-axis variable/facet variable combinations\n        # first we get the levels from original dataset\n        var_levels <- with(base_data, levels(get(facet_selected)))\n        # get all combinations of the factor name combined with the level name  \n        # (in the order that the levels are set)\n        factor_var_combinations <- paste0(facet_selected, var_levels)\n        # expand the counterfactual set to include appropriate combinations of \n        # the factor/level columns - all having range(0, 1, 1)\n        # first treat the initial counterfactual set explicitly as the x_axis\n        # cuts\n        x_axis_cuts <- counterfactuals\n        # then create the long, factor format of all x_axis/factor combos\n        # NOTE: sorts the facet_var column which messes up variable order\n        counterfactuals <- data.frame(x_axis_cuts, facet_var = \n                                          rep(factor_var_combinations, \n                                              each = length(x_axis_cuts))\n        )\n        # create the wide format (x_axis_cuts column gets dropped)\n        counterfactuals <- spread(counterfactuals, facet_var, x_axis_cuts)\n        # correct the variable order\n        counterfactuals <- counterfactuals[factor_var_combinations]\n        # convert the results to 0s and 1s\n        counterfactuals <- ifelse(is.na(counterfactuals), 0, 1)\n        # add the x_axis_cuts back in as the first column\n        counterfactuals <- data.frame(rep(x_axis_cuts), counterfactuals)\n        # drop the reference level\n        counterfactuals <- counterfactuals[, -2]\n        # label the x_axis_cuts column properly\n        names(counterfactuals)[1] <- x_axis_selected\n    } else {\n        # if no facet variable, simply expand the counterfactual vector to a\n        # one-column dataframe and label the column properly\n        counterfactuals <- expand.grid(counterfactuals)\n        names(counterfactuals) <- x_axis_selected\n    }\n    # finally, we check if there are additional predictors...\n    # (the \"formula\" call ensures we get the actual formula object rather\n    # than a reference to the object)\n    exp_formula <- formula(model_object$call[[2]])\n    variable_names <- all.vars(exp_formula)\n    predictor_names <- variable_names[-1]   # drop off the outcome variable\n    # we compare the total number of predictors against the number of columns\n    # in the counterfactual table...\n    if(length(predictor_names) > ncol(counterfactuals)) {\n        # if there are predictors not yet represented in the counterfactual set\n        # we define a regex search term that will match the x-axis and (if used)\n        # facet variables and we drop ALL partial and complete matches\n        # (getting rid of any interaction terms as well)\n        if(!is.null(facet_selected)) {\n            retained_index <- !grepl(paste(x_axis_selected, \n                                           facet_selected, \n                                           sep = \"|\"), \n                                     predictor_names)\n        } else {\n            retained_index <- !grepl(x_axis_selected, \n                                     predictor_names)\n        }\n        # we drop all the matches, leaving just the (non-interaction) \n        # predictors that we need to fix to a single value\n        extra_predictors <- predictor_names[retained_index]\n        # we quickly capture the current number of columns in our \n        # countefactual table and add one to it (giving us the index\n        # for where we are adding new columns)\n        offset_amount <- ncol(counterfactuals) + 1\n        \n        # now we get the means for the fixed predictors...\n        mean_set <- NULL\n        for(i in 1:length(extra_predictors)) {\n            mean_set[i] <- mean(as.numeric(exp_data[, extra_predictors[i]]), \n                                na.rm = T)\n        }\n        # and attach those means to the current counterfactual set\n        for(i in 1:length(extra_predictors)) {\n            counterfactuals <- cbind(counterfactuals, mean_set[i])\n        }\n        names(counterfactuals)[offset_amount:ncol(counterfactuals)] <- \n            extra_predictors\n    }\n    \n    # now we check if we have interaction columns in our predictors\n    if(!is.na(interaction_col_names)) {\n        # snag our column names and split terms\n        column_names <- interaction_col_names$column_name\n        split_terms <- interaction_col_names$split_term\n        # if we find them, we pull those terms out\n        interaction_index <- column_names %in% predictor_names\n        interaction_vars <- column_names[interaction_index]\n        # drop any split terms if needed\n        split_terms <- split_terms[interaction_index]\n        # create a list with the items in each term split based on our split\n        # terms\n        interaction_list <- list()\n        for(index in 1:length(interaction_vars)) {\n            current_split <- strsplit(interaction_vars[index], \n                                      split_terms[index], \n                                      perl = TRUE)\n            \n            interaction_list[[index]] <- current_split\n        }\n        # we quickly capture the current number of columns in our \n        # countefactual table and add one to it (giving us the index\n        # for where we are adding new columns)\n        offset_amount2 <- ncol(counterfactuals) + 1\n        # for each split term, we take the matching columns in the \n        # counterfactual table and multiply them together to create a new\n        # column for the interaction term\n        for(current_set in 1:length(interaction_list)) {\n            matching_names <- unlist(interaction_list[[current_set]])\n            matching_cols <- counterfactuals[matching_names]\n            new_col <- apply(matching_cols, 1, prod)\n            counterfactuals <- cbind(counterfactuals, new_col)\n        }\n        # we then give our interaction columns their proper names\n        names(counterfactuals)[offset_amount2:ncol(counterfactuals)] <- \n            interaction_vars\n    }\n    \n    # now we quickly reorder our new data object so that the columns match\n    # the order of our simulated coefficients objects\n    counterfactuals <- counterfactuals[\n        all.vars(exp_formula)[2:length(all.vars(exp_formula))]\n        ]\n    \n    # we wrap up by returning the counterfactual set\n    return(counterfactuals)\n}\n\n# Simulate expected probabilities using the new data and the coefficients:\n# This is a minor revision of Chris Adolph's mlogitsimev function from\n# his simcf package (https://github.com/chrisadolph/tile-simcf).\n#\n#       The function passes representative generated data to a collection of\n#       simulated coefficients. It then summarizes the results to return key\n#       features for each representative case passed to the cofficients:\n#       point estimate (mean), upper value (quantile based on given confidence \n#       interval), lower value (quantile based on given confidence interval).\n#\n#       The updated function (changes are marked with ## REVISION ##):\n#       1. Changes the point estimate technique from \"mean\" (as described above)\n#          to \"median\". This is perhaps a more common choice for this kind of\n#          simulation and avoids issues that arise as confidence intervals get\n#          narrow (e.g., the mean falling outside the upper and lower \n#          quartiles).\n#       2. Allows the user to request the un-summarized likelihoods that results \n#          from feeding the first case of represenative data to the coefficient \n#          estimates. This prevents the normal function behavior, which returns \n#          a summary of the likelihoods across all the coefficient sets for \n#          each case.\n#       3. Adjusts order of the outcome columns to match intuitive expectations\n#          (reference outcome is ordered as the FIRST column rather than LAST).\n#          Originally, the reference outcome was ordered as the LAST column, but\n#          in most models/outputs the reference outcome is ordered FIRST.\nMOS_mlogitsimev <- function (x, b, ci = 0.95, constant = 1, z = NULL, g = NULL, \n                             predict = FALSE, sims = 10, \n                             ## REVISION ##\n                             # added return_first_case_likelihoods argument - \n                             # if TRUE, the function will be interrupted after \n                             # feeding the first row of counterfactual data to  \n                             # the coefficients and we'll get back the \n                             # unsummarized likelihood for each coefficient set \n                             # for that single row\n                             return_first_case_likelihoods = FALSE) \n{\n    if (!is.array(b)) {\n        stop(\"b must be an array\")\n    }\n    if (any(class(x) == \"counterfactual\") && !is.null(x$model)) {\n        x <- model.matrix(x$model, x$x)\n        x <- array(x, dim = c(nrow(x), ncol(x), dim(b)[3]))\n    }\n    else {\n        if (any(class(x) == \"list\")) \n            x <- x$x\n        if (is.data.frame(x)) \n            x <- as.matrix(x)\n        if (!is.array(x)) {\n            if (!is.matrix(x)) {\n                x <- t(x)\n            }\n            x <- array(x, dim = c(nrow(x), ncol(x), dim(b)[3]))\n        }\n        else {\n            x <- array(x, dim = c(nrow(x), ncol(x), dim(b)[3]))\n        }\n        if (!is.na(constant)) {\n            xnew <- array(NA, dim = c(nrow(x), (ncol(x) + 1), \n                                      dim(b)[3]))\n            for (i in 1:dim(x)[3]) {\n                xnew[, , i] <- appendmatrix(x[, , i, drop = FALSE], \n                                            rep(1, dim(x)[1]), constant)\n            }\n            x <- xnew\n        }\n    }\n    if (!is.null(g)) {\n        usegamma <- TRUE\n    }\n    else {\n        usegamma <- FALSE\n    }\n    if (usegamma && !is.array(z)) {\n        stop(paste0(\"if g is provided, z must be an array with dimension 3 \",\n                    \"equal to the number of categories\"))\n    }\n    esims <- nrow(as.matrix(b))\n    res <- list(lower = array(0, dim = c(dim(x)[1], (dim(x)[3] + 1), \n                                         length(ci))), \n                upper = array(0, dim = c(dim(x)[1], (dim(x)[3] + 1), \n                                         length(ci)))\n    )\n    if (predict) \n        res$pv <- NULL\n    for (iscen in 1:dim(x)[1]) {\n        simdenom <- 0\n        for (icat in 1:(dim(b)[3])) {\n            if (usegamma) {\n                newdenom <- exp(b[, , icat] %*% x[iscen, , icat] + \n                                    g %*% z[iscen, , icat])\n                if(any(is.infinite(newdenom))) {\n                    stop(\"Getting unreasonable values (e.g., 'infinite') when \",\n                         \"trying to simulate data. Your model may be over fit.\")\n                }\n            } else {\n                newdenom <- exp(b[, , icat] %*% x[iscen, , icat])\n                if(any(is.infinite(newdenom))) {\n                    stop(\"Getting unreasonable values (e.g., 'infinite') when \",\n                         \"trying to simulate data. Your model may be over fit.\")\n                }\n            }\n            simdenom <- simdenom + newdenom\n        }\n        if (usegamma) {\n            simdenom <- simdenom + exp(g %*% z[iscen, , dim(z)[3]])\n        }\n        else {\n            simdenom <- simdenom + 1\n        }\n        simy <- matrix(NA, nrow = dim(b)[1], ncol = (dim(b)[3] + 1))\n        \n        for (icat in 1:dim(x)[3]) {\n            if (usegamma) \n                simy[, icat] <- exp(b[, , icat] %*% x[iscen, , icat] + \n                                        g %*% z[iscen, , icat])/simdenom\n            else simy[, icat] <- exp(b[, , icat] %*% x[iscen, , icat])/simdenom\n        }\n        if (usegamma) \n            simy[, ncol(simy)] <- exp(g %*% z[iscen, , dim(g)[3]])/simdenom\n        else simy[, ncol(simy)] <- 1/simdenom\n        \n        simy <- apply(simy, 2, sort)\n        \n        ## REVISION ##\n        # reorder columns so the REFERENCE OUTCOME is now the FIRST column\n        simy <- simy[, c(ncol(simy), 1:(ncol(simy) - 1))]\n        \n        ## REVISION ##\n        # if requested, return the first case likelihoods for each coefficient\n        # set\n        if(return_first_case_likelihoods) {\n            return(simy)\n        }\n        \n        ## REVISION ##\n        # technique for calculating point estimate (pe) changed from mean to\n        # median\n        res$pe <- rbind(res$pe, apply(simy, 2, median))\n        length.simy <- nrow(simy)\n        low <- up <- NULL\n        for (k in 1:length(ci)) {\n            for (icat in 1:(dim(b)[3] + 1)) {\n                res$lower[iscen, icat, k] <- \n                    rbind(low, quantile(simy[, icat], \n                                        probs = (1 - ci[k])/2))\n                res$upper[iscen, icat, k] <- \n                    rbind(up, quantile(simy[, icat], \n                                       probs = (1 - (1 - ci[k])/2)))\n            }\n        }\n        if (predict) {\n            pv <- NULL\n            for (ipred in 1:dim(b)[1]) {\n                pv <- c(pv, resample(1:dim(simy)[2], size = sims, \n                                     prob = simy[ipred, ], replace = TRUE))\n            }\n            res$pv <- rbind(res$pv, pv)\n            low <- up <- NULL\n            for (k in 1:length(ci)) {\n                for (icat in 1:(dim(b)[3] + 1)) {\n                    res$plower[iscen, icat, k] <- \n                        rbind(low, quantile(pv[, icat], \n                                            probs = (1 - ci[k])/2))\n                    res$pupper[iscen, icat, k] <- \n                        rbind(up, quantile(pv[, icat], \n                                           probs = (1 - (1 - ci[k])/2)))\n                }\n            }\n        }\n    }\n    res\n}\n\n###############################################################################\n## Functions to Visualize Outcome Likelihoods\n\nformat_for_ribbon_plot <- function(raw_likelihoods, \n                                   model_object, \n                                   base_data,\n                                   counterfactuals,\n                                   x_axis_selected = NA, \n                                   facet_selected = NULL,\n                                   explicit_outcome_order = NA) {\n    \n    # the mlogit structure is a collection of arrays but ggplot wants dataframes\n    # first we extract the arrays as matrices and bind them together\n    # NOTE: the lower/upper arrays will have as many dimensions as there are\n    #       confidence intervals (here we have 2 dimensions because we ask\n    #       for 95 and 50 percent CI in our mlogitsimev call)\n    num_col <- ncol(raw_likelihoods$lower)\n    tidy_sim <- rbind(matrix(raw_likelihoods$lower[, , 1], ncol = num_col),\n                      matrix(raw_likelihoods$lower[, , 2], ncol = num_col),\n                      matrix(raw_likelihoods$upper[, , 1], ncol = num_col),\n                      matrix(raw_likelihoods$upper[, , 2], ncol = num_col),\n                      matrix(raw_likelihoods$pe, ncol = num_col)\n    )\n    \n    # then we format the resulting collection to be properly grouped and\n    # labelled for visualizing\n    tidy_sim <- data.frame(tidy_sim)\n    # the outcome names are retained in the model object - we take these and\n    # label our prediction dataframe columns accordingly\n    names(tidy_sim) <- model_object$lab\n    # add a grouping variable for the three types of measures we get from\n    # the prediction object\n    tidy_sim$measure_type <- rep(c(\"lower95\", \"lower50\", \n                                   \"upper95\", \"upper50\", \n                                   \"pe\"), \n                                 each = nrow(raw_likelihoods$upper))\n    # if available, we also add the predictor (x-axis) value that will link the \n    # unique sets (lower, upper, pe) - this should naturally repeat to the \n    # appropriate length\n    if(!is.na(x_axis_selected)) {\n        tidy_sim$predictor <- counterfactuals[[x_axis_selected]]\n    } else {\n        # if no x-axis given, we just slap on a row-count\n        tidy_sim$predictor <- 1:nrow(counterfactuals)\n    }\n    # finally, if there is a facet variable set, we also add it as a grouping \n    # variable (create a new summary variable rather than deal with the \n    # already existing columns)\n    if(!is.null(facet_selected)) {\n        # we get the levels from the original data object\n        factor_levels <- levels(base_data[, facet_selected])\n        # the number of repitions of the factor is determined by the length\n        # of the x_axis variable / number of unique factor levels\n        num_reps <- nrow(raw_likelihoods$upper) / length(factor_levels)\n        # finally add the grouping variable\n        tidy_sim$facet <- rep(factor_levels, each = num_reps)\n    }    \n    \n    # collapsing and spreading variables to make visualizing easy\n    # (this is a tad arbitrary - it is consisent with Brian's interpretation of\n    # good ggplot practice)\n    if(!is.null(facet_selected)) {\n        # if a facet variable is set, respect it...\n        tidy_sim <- gather(tidy_sim, outcome, likelihood, -measure_type, \n                           -predictor, -facet)    \n    } else {\n        # otherwise don't because it's not there\n        tidy_sim <- gather(tidy_sim, outcome, likelihood, -measure_type, \n                           -predictor)\n    }\n    tidy_sim <- spread(tidy_sim, measure_type, likelihood)\n    \n    # check if an explicit order is provided for the levels of the \"outcome\"\n    # variable - apply it if given\n    if(!is.na(explicit_outcome_order)) {\n        tidy_sim$outcome <- factor(tidy_sim$outcome, explicit_outcome_order)\n    }\n    \n    # now we need to take our well formed data object and switch it to a long\n    # format so that it is ready for the geom_ribbon needs\n    \n    if(is.null(facet_selected)) {\n        id_columns <- c(\"predictor\", \"outcome\")\n    } else {\n        id_columns <- c(\"predictor\", \"outcome\", \"facet\")\n    }\n    \n    fl_long <- tidy_sim %>%\n        # we want to work with the lower/upper values separately, so we first \n        # remove one half of the values (here the upper) and the unneeded pe \n        # column\n        dplyr::select(-pe, -upper95, -upper50) %>%\n        # then we melt our data frame so that the 50/95 values are in the same\n        # column\n        melt(id.vars = id_columns, \n             value.name = \"lower_values\", \n             variable.name = \"ci\") %>%\n        # now we extract the 50/95 from the 50/95 levels by dropping all the \n        # character values from the string\n        mutate(ci = as.numeric(gsub(pattern = \"^[a-z]*\", \n                                    x = ci, \n                                    replacement = \"\"))) %>%\n        # now we repeat for the other set of values (upper) and then join the\n        # results into a single dataframe with our lower and upper values in \n        # their own columns\n        left_join(\n            tidy_sim %>%\n                dplyr::select(-pe, -lower95, -lower50) %>%\n                melt(id.vars = id_columns, \n                     value.name = \"upper_values\", \n                     variable.name = \"ci\") %>%\n                mutate(ci = as.numeric(gsub(pattern = \"^[a-z]*\", \n                                            x = ci, \n                                            replacement = \"\"))),\n            by = c(id_columns, \"ci\")) %>%\n        # and we wrap up by adding an interaction variable so that our ggplot\n        # group correctly recognizes that we want to group by both outcome and \n        # confidence interval group (50 or 95)\n        mutate(group = interaction(ci, outcome))\n    \n    # returning our visualization-ready data\n    return(fl_long)\n}\n\nget_ribbon_plot <- function(formatted_likelihoods,\n                            facet_selected = NULL,\n                            plot_title = \"\",\n                            x_lab = \"Predictor\", \n                            y_lab = \"Probability of Outcome\",\n                            custom_colors = NULL,\n                            custom_breaks = NULL,\n                            custom_labels = NULL) {\n    \n    plot_object <- ggplot(formatted_likelihoods, \n                          aes(x = predictor, group = group)) +\n        # the geoms\n        geom_ribbon(aes(fill = outcome, \n                        ymin = lower_values, ymax = upper_values,\n                        alpha = factor(ci))) +\n        # label adjustements\n        labs(x = x_lab, y = y_lab) +\n        ggtitle(plot_title) +\n        # scale adjustments\n        scale_alpha_manual(values = c(0.4, 0.5), guide = FALSE) +\n        scale_y_continuous(limits = c(0, 1),\n                           labels = percent,\n                           expand = c(0, 0)) +\n        # theme adjustments\n        MOS_theme +\n        guides(fill = guide_legend(title = NULL))\n    \n    # now we want to customize the x-axis but how we do this will depend on\n    # how custom_breaks and custom_labels are configured\n    # 1. custom breaks, no custom labels\n    if(!is.null(custom_breaks) & is.null(custom_labels)) {\n        plot_object <- plot_object + \n            scale_x_continuous(expand = c(0, 0),\n                               breaks = custom_breaks)\n    # 2. custom labels, no custom breaks\n    } else if(is.null(custom_breaks) & !is.null(custom_labels)) {\n        plot_object <- plot_object +\n            scale_x_continuous(expand = c(0, 0), \n                               labels = custom_labels)\n    # 3. both customized\n    } else if(!is.null(custom_breaks) & !is.null(custom_labels)) {\n        plot_object <- plot_object +\n            scale_x_continuous(expand = c(0, 0),\n                               breaks = custom_breaks,\n                               labels = custom_labels)\n    # 4. neither customized\n    } else {\n        plot_object <- plot_object +\n            scale_x_continuous(expand = c(0, 0))\n    }\n\n    # if custom colors are provided, adjust the color scale and update theme\n    if(!is.null(custom_colors)) {\n        plot_object <- plot_object + \n            scale_fill_manual(values = custom_colors)\n    }\n    \n    # if a facet variable is set, add the facet layer to the plot object\n    if(!is.null(facet_selected)) {\n        plot_object <- plot_object + \n            facet_wrap(~ facet, ncol = 3) +\n            theme(panel.margin = unit(1, \"lines\"),\n                  strip.text.x = element_text(size = 8)\n            )\n        # if custom x-axis ticks are provided, also want to tweak the x-axis\n        # tick text and orientation to minimize collisions/overlap between\n        # facets\n        if(!is.null(custom_labels)) {\n            plot_object <- plot_object +\n                theme(axis.text.x = element_text(size = 8, angle = 45, \n                                                 hjust = 1, vjust = 1)\n                )\n        }\n    }\n    \n    # return the plot object\n    return(plot_object)\n}\n\nget_dot_cloud_plot <- function(formatted_likelihoods,\n                               x_lab = \"Simulated Outcome Probability\", \n                               y_lab = \"\",\n                               custom_colors = NULL) {\n    # build the base plot object\n    plot_object <- ggplot(formatted_likelihoods, \n                          aes(x = outcome, y = single_pe,\n                              color = outcome, alpha = 0.10)) + \n        # the geoms\n        geom_jitter(position = position_jitter(width = 0.25, height = 0)) +\n        # label adjustments\n        labs(title = \"The Likelihood of Each Outcome Across 1000 Simulations\",\n             x = x_lab, y = y_lab) +\n        # scale adjustments\n        scale_x_discrete(limits = rev(levels(formatted_likelihoods$outcome))) +\n        scale_y_continuous(limits = c(0, 1),\n                           labels = scales::percent) +\n        # theme adjustments\n        MOS_theme +\n        theme(legend.position=\"none\") +\n        coord_flip()\n    \n    # if custom colors are provided, adjust the color scale and update theme\n    if(!is.null(custom_colors)) {\n        plot_object <- plot_object + \n            scale_color_manual(values = custom_colors)\n    }\n    \n    # return the plot object\n    return(plot_object)\n}\n\n###############################################################################\n## Assorted Helper Functions\n\n# Extract the fixed ui options (the levels for any ui features that are\n# generated statically - such as the x-axis choices - rather than dynamically -\n# such as the sliders).\nget_fixed_ui_options <- function(variable_config_list) {\n    # collect the x-axis options names and definitions\n    x_axis_options <- c()\n    x_axis_definitions <- c()\n    for(index in 1:length(variable_config_list)) {\n        if(variable_config_list[[index]]$x_axis_candidate) {\n            current_name       <- variable_config_list[[index]]$pretty_name\n            current_def        <- variable_config_list[[index]]$definition\n            x_axis_options     <- c(x_axis_options, current_name)\n            x_axis_definitions <- c(x_axis_definitions, current_def)\n        }\n    }\n    \n    # collect the facet options names and definitions\n    facet_options <- c()\n    facet_definitions <- c()\n    for(index in 1:length(variable_config_list)) {\n        if(variable_config_list[[index]]$facet_candidate) {\n            current_name      <- variable_config_list[[index]]$pretty_name\n            current_def       <- variable_config_list[[index]]$definition\n            facet_options     <- c(facet_options, current_name)\n            facet_definitions <- c(facet_definitions, current_def)\n        }\n    }\n    \n    # return all option collections\n    list(x_axis_options     = x_axis_options, \n         x_axis_definitions = x_axis_definitions,\n         facet_options      = facet_options,\n         facet_definitions  = facet_definitions)\n}\n\n# We need to be able to identify columns that are the result of interactions\n# between base variables. However, the base data expansion makes it difficult\n# to recover these columns - it also expands factor levels and does not use\n# a unique separator for interaction combinations v. factor levels. This\n# function finds the possible name permutations that may result from\n# interactions and identifies columns that have matches - which will be the\n# interaction columns.\nget_interaction_col_names <- function(base_formula, exp_data) {\n    # convert the base formula into a single character string (ignoring the \n    # outcome variable)\n    formula_string <- as.character(base_formula)[[3]]\n    \n    # parse all the separate terms (not variables but rather anything that\n    # occurs before or after a \"+\" symbol)\n    formula_parsed <- strsplit(formula_string, \"+\", fixed = TRUE)\n    \n    # get rid of any spaces (note that strsplit returns a list - we unlist to \n    # make sure we work with a character vector)\n    formula_parsed <- gsub(\" \", \"\", unlist(formula_parsed))\n    \n    # now we collect just those terms that have an interaction symbol \"*\"\n    # TODO check for : to mark interactions\n    interaction_terms <- grep(\"\\\\*|:\", formula_parsed, value = TRUE)\n    \n    # quickly check to see if there are any interaction terms at all - if none\n    # we want to return \"NA\" so that later steps in this function don't fail\n    # and so that we can identify the lack of interaction terms correctly in\n    # later functions\n    if(length(interaction_terms) == 0) {\n        return(NA)\n    }\n    \n    # for each term, we extract the variable names\n    interaction_terms <- strsplit(interaction_terms, \"\\\\*|:\")\n    \n    # for each term, we need to construct all possible combinations of the \n    # variable names (variable names separated by a \".\" and - for factor-factor\n    # interactions - possibly some other \".\" and text) - these are what we \n    # will use to identity the interaction data columns in the expanded data \n    # frame\n    # 1. get the permutations of the variable name strings\n    interaction_combos <- lapply(interaction_terms, permn)\n    # 2. build the regex search strings to test if there are column matches \n    #    for each permutation (first term will always start the column name,\n    #    second term will always occur somewhere in the string immediately\n    #    after a period)\n    reg_set <- c()\n    \n    for(combo_index in 1:length(interaction_combos)) {\n        current_combo <- interaction_combos[[combo_index]]\n        reg_combo <- list()\n        \n        for(subset_index in 1:length(current_combo)) {\n            current_subset <- current_combo[[subset_index]]\n            \n            # first pattern\n            first_pattern <- c(paste0(\"^\", current_subset[[1]]))\n            \n            # second pattern (note that we only need to make up to the two-\n            # way match - all three-way+ matches will be matched by a two-way;\n            # also note that we match the PERIOD that occurs before the \n            # variable name - this let's us re-use this term to split\n            # the name later on)\n            second_pattern <- paste0(\"\\\\.(?=\", \n                                     current_subset[[2]], \n                                     \"[a-zA-Z])|\",\n                                     \"\\\\.(?=\", current_subset[[2]], \"$)\")\n            \n            reg_combo[[subset_index]] <- c(first_pattern, second_pattern)\n        }\n        \n        reg_set[[combo_index]] <- reg_combo\n    }\n    \n    # 3. test the regex pairs against the exp_data names to see which columns\n    #    are interaction columns\n    interaction_test_collection <- lapply(reg_set, function(x) {\n        pair_collection <- list()\n        for(index in 1:length(x)) {\n            current_pair <- x[[index]]\n            pair_collection[[index]] <- grepl(current_pair[[1]], \n                                              names(exp_data),\n                                              perl = TRUE) & \n                grepl(current_pair[[2]], \n                      names(exp_data),\n                      perl = TRUE)\n        }\n        \n        return(pair_collection)\n    })\n    \n    # 4. collapse the test collection so that it is a single index - any column\n    #    that had one or more matches to a test pair is an interaction column\n    interaction_matrix <- matrix(unlist(interaction_test_collection), \n                                 ncol = length(names(exp_data)), \n                                 byrow = TRUE)\n    \n    interaction_index <- apply(interaction_matrix, 2, any)\n    \n    # 5. get the subset of column names that match a permutation\n    interaction_col_names <- names(exp_data)[interaction_index]\n    \n    # 6. get the regex terms that will allow us to split the columns (these\n    #    may be in a funny order)\n    reg_set_matches <- apply(interaction_matrix, 1, any)\n    \n    reg_set_matrix <- matrix(unlist(reg_set), ncol = 2, byrow = T)\n    \n    split_terms <- reg_set_matrix[,2][reg_set_matches]\n    \n    # 6. pair the column names with their regex split terms (fix any order\n    #    issues)\n    # NOTE: this first step gets just the first matching position - if the same\n    #       variable is the second of multiple interaction pairs, multiple \n    #       values will be returned; this is not an issue because of how we\n    #       do the next step\n    split_term_order <- sapply(split_terms, function (x) \n        grep(x, interaction_col_names, perl = TRUE)[1])\n    \n    #       recreating the terms collection in the correct order (this will fill\n    #       in duplicate matches as needed)\n    split_terms <- split_terms[c(split_term_order)]\n    \n    # 7. join the terms with the col names\n    interaction_cols <- data.frame(column_name = interaction_col_names, \n                                   split_term = split_terms,\n                                   stringsAsFactors = FALSE)\n    \n    # return the collection of names and split terms\n    return(interaction_cols)\n}\n\n# This function expands the variable configuration object to include some more\n# features, specifically those needed to define the sliders. It calculates\n# these from the base data object.\nadd_input_features <- function(variable_config_object, base_data) {\n    # loop over the variables specified the variable configuration object\n    for(index in 1:length(variable_config_object)) {\n        # adjust object name to be more manageable\n        vc <- variable_config_object\n        \n        # grab the current variable raw name\n        current_var <- names(vc)[[index]]\n        \n        # if it's numeric, calculate the relevant values, otherwise assign NA\n        # to the values so the properties exist but are appropriate for a non-\n        # numeric variable\n        if(is.numeric(base_data[[current_var]])) {\n            current_median <- median(base_data[[current_var]])\n            current_range  <- range(base_data[[current_var]])\n            \n            # apply the variable's transform_to_ui function (making the values\n            # ui friendly)\n            current_median   <- vc[[index]]$transform_for_ui(current_median)\n            current_range[1] <- vc[[index]]$transform_for_ui(current_range[1])\n            current_range[2] <- vc[[index]]$transform_for_ui(current_range[2])\n            \n            # in case of reverse transformations, make sure range is ordered\n            # ascending\n            current_range <- sort(current_range)\n            \n            # round the range values so that ugly values are more ui friendly\n            current_range[1] <- floor(current_range[1])\n            current_range[2] <- ceiling(current_range[2])\n        } else {\n            current_median <- NA\n            current_range <- NA\n        }\n        \n        # if median/range are NA, grab the variable levels, otherwise set to NA\n        if(is.na(current_median)) {\n            current_levels <- levels(base_data[[current_var]])\n        } else {\n            current_levels <- NA\n        }\n        \n        # add the values to the variable configuration\n        variable_config_object[[current_var]]$ui_median <- current_median\n        variable_config_object[[current_var]]$ui_min <- current_range[1]\n        variable_config_object[[current_var]]$ui_max <- current_range[2]\n        variable_config_object[[current_var]]$ui_levels <- current_levels\n    }\n    \n    # return the update variable_config_object\n    return(variable_config_object)\n}\n\n# This function generates the actual slider objects. It expects the variable\n# configuration object to determine which variables to make sliders for, but\n# will also accept a vector of raw variable names that overrides the variable \n# configuration file to exclude selected variables. A unique \"append\" name must\n# also be provided to ensure that separate slider sets to not share name space.\nmake_inputs <- function(variable_config_list, \n                        variables_to_drop = NA,\n                        append_name,\n                        return_sliders,\n                        return_facets) {\n    # if sliders are desired, we'll make a slider for each slider candidate\n    # (excluding any variables listed in \"variables to drop\")\n    if(return_sliders) {\n        # index which variables are slider candidates\n        slider_index <- unlist(lapply(variable_config_list, \n                                      function(x) x$slider_candidate == TRUE))\n        \n        # if a vector of variables to drop has been provided, adjust their indices \n        # to FALSE so sliders are not made for them (this is primarily useful for\n        # dropping the x-axis variable where needed)\n        if(!is.na(variables_to_drop)) {\n            for(index in 1:length(variables_to_drop)) {\n                slider_index[variables_to_drop[index]] <- FALSE\n            }\n        }\n        \n        # if there are no slider candidates, we skip creating a slider set and\n        # set the slider_set value so that it is handled properly later\n        if(!any(slider_index)) {\n            slider_set = NULL\n        } else {\n            \n            # subset variable_config_list to just get the slider candidates\n            # (excluding the x-axis variabe)\n            selected_sliders <- variable_config_list[slider_index]\n            \n            # generate the sliders (and their popovers)\n            slider_set <- lapply(1:length(selected_sliders), function(i) {\n                popify(\n                    sliderInput(\n                        inputId = paste0(append_name,\n                                         \"_\",\n                                         names(selected_sliders)[i]), \n                        label   = selected_sliders[[i]]$pretty_name,\n                        min     = selected_sliders[[i]]$ui_min, \n                        max     = selected_sliders[[i]]$ui_max,\n                        value   = selected_sliders[[i]]$ui_median,\n                        step    = ifelse(is.na(selected_sliders[[i]]$slider_rounding),\n                                         0.01,\n                                         selected_sliders[[i]]$slider_rounding)),\n                    \n                    title     = selected_sliders[[i]]$pretty_name,\n                    content   = selected_sliders[[i]]$definition,\n                    placement = \"bottom\",\n                    trigger   = \"hover\"\n                )\n                \n            })\n        }\n    }\n    \n    # if facets are desired, we'll make drop-downs for each of those as well\n    # (only really appropriate for Single Case mode)\n    if(return_facets) {\n        # index which variables are facet candidates\n        facet_index <- unlist(lapply(variable_config_list, \n                                     function(x) x$facet_candidate == TRUE))\n        \n        # if a vector of variables to drop has been provided, adjust their  \n        # indices to FALSE so facets are not made for them\n        if(!is.na(variables_to_drop)) {\n            for(index in 1:length(variables_to_drop)) {\n                facet_index[variables_to_drop[index]] <- FALSE\n            }\n        }\n        \n        # if there are no dropdown candidates, we skip creating dropdowns and\n        # set the dropdown_set value so that it is handled properly later\n        if(!any(facet_index)) {\n            facet_set = NULL\n        } else {\n            # subset variable_config_list to just get the facet candidates\n            selected_facets <- variable_config_list[facet_index]\n            \n            # generate the dropdowns (and their popovers)\n            facet_set <- lapply(1:length(selected_facets), function(i) {\n                selectInput(\n                    inputId = paste0(append_name,\n                                     \"_\",\n                                     names(selected_facets)[i]), \n                    label   = selected_facets[[i]]$pretty_name,\n                    choices = selected_facets[[i]]$ui_levels)\n            })\n        }\n    }\n    \n    if(exists(\"slider_set\") & exists(\"facet_set\")) {\n        return(c(slider_set, facet_set))\n    } else if(exists(\"slider_set\")) {\n        return(slider_set)   \n    } else {\n        return(facet_set)\n    }\n}\n\n# This function updates the base data object to create a data object adjusted\n# for appropriate slider values. The inputs to the function should echo the\n# inputs used to make the slider set with the addition of specifying the \n# target object. To establish the reactive link, we also need to explicitly\n# pass the input object.\napply_input_values <- function(update_target,\n                               interaction_col_names,\n                               variable_config_list,\n                               input_call,\n                               append_name,\n                               base_data,\n                               use_slider_values = TRUE,\n                               use_dropdown_values = FALSE,\n                               variables_to_drop = NA\n) {\n    # index which variables are input candidates - the way we build this index\n    # will depend on which input source(s) we want to apply\n    if(use_slider_values == TRUE & use_dropdown_values == FALSE) {\n        input_index <- unlist(lapply(variable_config_list, \n                                     function(x) x$slider_candidate == TRUE))\n    } else if(use_slider_values == TRUE & use_dropdown_values == TRUE) {\n        input_index <- unlist(lapply(variable_config_list, \n                                     function(x) x$slider_candidate == TRUE ||\n                                         x$facet_candidate == TRUE))\n    } else {\n        input_index <- unlist(lapply(variable_config_list, \n                                     function(x) x$facet_candidate == TRUE))\n    }\n    \n    # if a vector of variables to drop has been provided, adjust their indices \n    # to FALSE so inputs are not made for them (this is primary use for this\n    # is dropping the x-axis variable when needed)\n    if(!is.na(variables_to_drop)) {\n        for(index in 1:length(variables_to_drop)) {\n            input_index[variables_to_drop[index]] <- FALSE\n        }\n    }\n    \n    # subset variable_config_list to just get the input candidates\n    selected_inputs <- variable_config_list[input_index]\n    \n    # update the values based on current slider inputs\n    for(i in 1:length(selected_inputs)) {\n        # grab the key details\n        current_var <- names(selected_inputs)[[i]]\n        input_name <- paste0(append_name, \"_\", current_var)\n        input_value <- isolate(input_call[[input_name]])\n        input_type <- ifelse(selected_inputs[[i]]$slider_candidate,\n                             \"slider\",\n                             \"dropdown\")\n        input_trans <- selected_inputs[[current_var]]$transform_for_model \n        \n        # all the input variables initialize as \"NULL\" - we want to avoid\n        # working with them until they've been assigned a value so we simply\n        # return the update_target in that case\n        # NOTE: input_call[[current_var]] IS a reactive link (actually a  \n        #       flexible set of reactive links) - it links to the inputs \n        #       dynamically generated by the output$APPEND_input_set observers\n        if(is.null(isolate(input_call[[input_name]]))) {\n            return(update_target)\n        }\n        \n        # apply the relevant transformation to convert slider values\n        # to model-appropriate values\n        model_value <- input_trans(input_value)\n        \n        # at this point, we apply the input value differently if the value came\n        # from a slider or a dropdown;\n        # the expanded dataset includes a single column for each slider\n        # variable + appropriate interaction columns\n        # in contrast, the dataset includes a column for each dropdown level \n        # except the reference level (the first factor level however the levels \n        # were ordered in base_data) + appropriate interaction columns for EACH\n        # non-reference levels\n        if(input_type == \"slider\") {\n            # update the matching update_target column with the \n            # current slider value\n            update_target[current_var] <- model_value\n        } else if (input_type == \"dropdown\") {\n            # we are going to have to update ALL of the non-interaction\n            # columns associated with this factor - we want all the non-\n            # selected levels to be set to \"0\" (not true) and the selected\n            # level to be set to \"1\" (true)\n            \n            # we snag the variable levels from the base data\n            var_levels <- levels(base_data[[current_var]])\n            # we replace any non-alphanumeric values with \".\" to match the \n            # subsitution that occurs during the expansion from base_data to \n            # exp_data\n            var_levels <- gsub(\"[^[:alnum:]]\", \".\", var_levels)\n            # we create the non-interaction column names by combining the\n            # raw variable name with the level names\n            single_col_names <- paste(current_var, var_levels, sep = \"\")\n            # we drop the reference level\n            single_col_names <- single_col_names[-1]\n            \n            # we define the column that matches our selected factor level\n            matching_col <- paste(current_var,\n                                  gsub(\"[^[:alnum:]]\", \".\", input_value),\n                                  sep = \"\")\n            \n            # we initially set all remaining the columns to \"0\"\n            for(current_col in single_col_names) {\n                update_target[current_col] <- 0\n            }\n            \n            # finally, we verify that our matching column is present in the\n            # expanded data set - if it is, we update it; if it isn't, it's the\n            # reference level and we stop since we have set all non-reference\n            # levels to \"0\"\n            if(any(grepl(matching_col, names(update_target), fixed = TRUE))) {\n                update_target[matching_col] <- 1\n            }\n        }\n    }\n    \n    # now we quickly refresh all of the interaction columns (just doing this by\n    # default is, on average, faster than testing if each input is related to\n    # an interaction; it is also technically much easier to implement; this\n    # claim may not hold if there are a large number of interactions)\n    if(!is.na(interaction_col_names)) {\n        # create a list of the interaction column names split in half (by their\n        # matching split term)\n        column_names <- interaction_col_names$column_name\n        split_terms <- interaction_col_names$split_term\n        interaction_list <- list()\n        for(index in 1:length(column_names)) {\n            current_split <- strsplit(column_names[index], \n                                      split_terms[index], \n                                      perl = TRUE)\n            \n            interaction_list[[index]] <- current_split\n        }\n        \n        # update the interaction variables by multiplying their\n        # source columns together\n        for(current_set in 1:length(interaction_list)) {\n            matching_names <- unlist(interaction_list[[current_set]])\n            matching_cols <- update_target[matching_names]\n            updated_col <- apply(matching_cols, 1, prod)\n            update_target[column_names[[current_set]]] <- updated_col\n        }\n    }\n    \n    return(update_target)\n}\n\n# We need to summarize the key details of each ribbon plot. This builds a text\n# string (with HTML formatting) that can be used for that purpose.\nbuild_ribbon_summary <- function(x_axis_raw_name,\n                                 facet_raw_name,\n                                 variable_config_list,\n                                 include_plot_summary) {\n    \n    # extract the matching x-axis config variable\n    x_axis_target <- variable_config_list[[x_axis_raw_name]]\n    \n    # if a facet variables has been selected, we exract that config variable\n    # as well\n    if(!is.null(facet_raw_name)) {\n        facet_target <- variable_config_list[[facet_raw_name]]\n    } else {\n        facet_target <- NA\n    }\n    \n    # combine the relevant variable features to make the summary\n    if(!is.na(facet_target)) {\n        ribbon_defs <- paste0(\n            \"<strong>\", x_axis_target$pretty_name, \"</strong><br>\",\n            x_axis_target$definition, \"<br><br>\",\n            \n            \"<strong>\", facet_target$pretty_name, \"</strong><br>\",\n            facet_target$definition, \"<br><br>\"\n        )\n    } else {\n        ribbon_defs <- paste0(\n            \"<strong>\", x_axis_target$pretty_name, \"</strong><br>\",\n            x_axis_target$definition, \"<br><br>\"\n        )\n    }\n    \n    if(include_plot_summary) {\n        ribbon_summary <- paste0(\n            ribbon_defs,\n            \"<strong>Key Trends for the X-Axis Variable</strong><br>\",\n            x_axis_target$ribbon_plot_summary\n        )\n    } else {\n        ribbon_summary <- paste0(\n            ribbon_defs,\n            \"<strong>Key Trends for the X-Axis Variable</strong><br>\",\n            \"Not available when using Advanced Options.\")\n    }\n    \n    # return the text string\n    return(ribbon_summary)\n}\n\n# We need to be able to simply return the collections of inputs. This will\n# allow us to test for when inputs in the collection have changed. The primary\n# use for this is adjusting the Update/Simulate buttons to get user attention.\nreturn_inputs <- function(variable_config_list,\n                          input_call,\n                          append_name,\n                          base_data,\n                          use_slider_values = TRUE,\n                          use_dropdown_values = FALSE,\n                          variables_to_drop = NA) {\n    # index which variables are input candidates - the way we build this index\n    # will depend on which input source(s) we want to apply\n    if(use_slider_values == TRUE & use_dropdown_values == FALSE) {\n        input_index <- unlist(lapply(variable_config_list, \n                                     function(x) x$slider_candidate == TRUE))\n    } else if(use_slider_values == TRUE & use_dropdown_values == TRUE) {\n        input_index <- unlist(lapply(variable_config_list, \n                                     function(x) x$slider_candidate == TRUE ||\n                                         x$facet_candidate == TRUE))\n    } else {\n        input_index <- unlist(lapply(variable_config_list, \n                                     function(x) x$facet_candidate == TRUE))\n    }\n    \n    # if a vector of variables to drop has been provided, adjust their indices \n    # to FALSE so inputs are not made for them (this is primary use for this\n    # is dropping the x-axis variable when needed)\n    if(!is.na(variables_to_drop)) {\n        for(index in 1:length(variables_to_drop)) {\n            input_index[variables_to_drop[index]] <- FALSE\n        }\n    }\n    \n    # subset variable_config_list to just get the input candidates\n    selected_inputs <- variable_config_list[input_index]\n    \n    # return the current slider values (triggering reactive link to each)\n    input_values <- c()\n    for(i in 1:length(selected_inputs)) {\n        # grab the key details\n        current_var <- names(selected_inputs)[[i]]\n        input_name <- paste0(append_name, \"_\", current_var)\n        input_values <- c(input_values, input_call[[input_name]])\n    }\n    \n    # return the values to avoid funky return errors\n    return(input_values)\n}\n\n###############################################################################\n## END OF SCRIPT\n###############################################################################",
    "created" : 1441218739248.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3664441689",
    "id" : "AF439313",
    "lastKnownWriteTime" : 1436386742,
    "path" : "C:/Projects/COS/MOS_packages_and_custom_functions.R",
    "project_path" : "MOS_packages_and_custom_functions.R",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_source"
}